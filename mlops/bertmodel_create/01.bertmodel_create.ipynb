{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TinyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/laxmimerit/All-CSV-ML-Data-Files-Download/master/IMDB-Dataset.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/dev/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'sentiment'],\n",
       "        num_rows: 35000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review', 'sentiment'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "dataset = Dataset.from_pandas(data)\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.3)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    25000\n",
       "negative    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 35000/35000 [00:01<00:00, 30882.98 examples/s]\n",
      "Map: 100%|██████████| 15000/15000 [00:00<00:00, 41542.49 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'sentiment', 'label'],\n",
       "        num_rows: 35000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review', 'sentiment', 'label'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = {\"negative\": 0, \"positive\": 1}\n",
    "dataset = dataset.map(lambda x: {\"label\": label2id[x[\"sentiment\"]]})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': ['Man, what a scam this turned out to be! Not because it wasn\\'t any good (as I wasn\\'t really expecting anything from it) but because I was misled by the DVD sleeve which ignorantly paraded its \"stars\" as being Stuart Whitman, Stella Stevens and Tony Bill. Sure enough, their names did not appear in the film\\'s opening credits, much less themselves in the rest of it!! As it turned out, the only movie which connects those three actors together is the equally obscure LAS VEGAS LADY (1975) \\x96 but what that one has to do with THE CRATER LAKE MONSTER is anybody\\'s guess\\x85 <br /><br />Even so, since I paid $1.50 for its rental and I was in a monster-movie mood anyhow, I elected to watch the movie regardless and, yup, it stunk! Apart from the fact that it had a no-name cast and an anonymous crew, an unmistakably amateurish air was visible from miles away and the most I could do with it is laugh at the JAWS-like pretensions and, intentionally so, at the resistible antics of two moronic layabouts-cum-boat owners who frequently squabble among themselves with the bemused local sheriff looking on. The creature itself \\x96 a plesiosaur i.e. half-dinosaur/half-fish \\x96 is imperfectly realized (naturally) but, as had been the case with THE GIANT CLAW (1957) which I\\'ve also just seen, this didn\\'t seem to bother the film-makers none as they flaunt it as much as they can, especially during the movie\\'s second half!',\n",
       "  'I bought this DVD without any previous reference but the names of John Huston, Raquel Welch, Mae West and Farrah Fawcett on its cover. I found the Brazilian title very weird, but I decided to watch expecting to see a funny comedy maybe like \"Switch\". However the non-sense story is awful and hard to be described. Myron Breckinridge (Rex Reed) is submitted to a surgery to change his sex in Copenhagen and he returns to Hollywood telling that she is to be Myra Breckinridge (Raquel Welch) and claiming half the property of his uncle Buck Loner (John Huston). Along the days, Myra and her alter-ego Myron corrupt a young couple in her uncle\\'s academy with kinky sex. In a certain moment, the messy screenplay is so confused that I believe the whole story was only a mind trip of Myron induced by the accident. Unfortunately the beauties of Raquel Welch and Farrah Fawcett are not enough to hold this flick. My vote is three.<br /><br />Title (Brazil): \"Homem & Mulher Até Certo Ponto\" (\"Man & Woman Up to a Point\")',\n",
       "  'The story of Cinderella is one of my favorites from Charles Perrault, with Sleeping Beauty which was also made into a Disney film in 1959; this film is a sweet, enchanting masterpiece from Disney.<br /><br />The film has a great soundtrack; that\\'s one I like in a movie is a very good soundtrack, and I love the songs too; my favorite song is the romantic \"So This is Love.\" I love the mice from the film too, they are cute. My favorite scene is the scene after the narration, the little birds tried to wake Cinderella up in the morning; I also love it when Cinderella\\'s animal friends (the mice and birds) fix up Cinderella\\'s birth-mother\\'s dress, so she could go to the ball...until Drizella & Anastasia tore it to bits, the b****es!',\n",
       "  \"Yes, it's pure trash. It might be interesting for every guy who likes experimental cinema (like me) to see lowlifes babbling and doing nothing for almost two hours, but it gets very painful when you realize you have actually paid for this. Probably, this is one of those films you love to watch for its complete emptiness and nihilism. I accept it though for its shock value, decades before Trainspotting and Pulp Fiction.\",\n",
       "  'So lets say you are a producer, you have some money, and you somehow got Richard gear and Bruce Willis, so now all you need is a script...but , why bother? see, this movie is really terrible, the acting is pretty good, but the casting is awful gear and Willis did their best to play these characters they simply cant play. now, this movie has no plot, or rather, there\\'s something that tries to pass off as a plot: there\\'s a mean hit-man (Willis) who is cool dangerous and sophisticated (actually he\\'s none of those things, but from some dialog between the other characters, you are supposed to get this impression), so this hit-man is on a mission to kill someone, now there\\'s an ex-IRA prisoner who is kind and nice and very likable (gear) who the FBI release from prison so he will help them. so now the FBI is \"investigating\" to find this hit-man, the investigation consists of a series of unlikely information - like some random person lost his wallet and someone used his name to buy a car - which is always right on the money. since there\\'s no real plot and the script is so crappy, you cant expect any real character development, or tension, so instead of creating them through the story, they simply add some disconnected dramatic music, which signals you that something very dramatic is going on , instead of actually creating something dramatic..<br /><br />the bottom line is that this is a very bad movie, and a complete waste of time which somehow got an incredibly high score for its level, probably because of the cast - and this is exactly what the producers where counting on.'],\n",
       " 'sentiment': ['negative', 'negative', 'positive', 'negative', 'negative'],\n",
       " 'label': [0, 0, 1, 0, 0]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/dev/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='huawei-noah/TinyBERT_General_4L_312D', vocab_size=30522, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "model = \"huawei-noah/TinyBERT_General_4L_312D\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2158, 1010, 2054, 1037, 8040, 3286, 2023, 2357, 2041, 2000, 2022, 999, 2025, 2138, 2009, 2347, 1005, 1056, 2151, 2204, 1006, 2004, 1045, 2347, 1005, 1056, 2428, 8074, 2505, 2013, 2009, 1007, 2021, 2138, 1045, 2001, 28616, 3709, 2011, 1996, 4966, 10353, 2029, 21591, 2135, 7700, 2094, 2049, 1000, 3340, 1000, 2004, 2108, 6990, 21311, 1010, 11894, 8799, 1998, 4116, 3021, 1012, 2469, 2438, 1010, 2037, 3415, 2106, 2025, 3711, 1999, 1996, 2143, 1005, 1055, 3098, 6495, 1010, 2172, 2625, 3209, 1999, 1996, 2717, 1997, 2009, 999, 999, 2004, 2009, 2357, 2041, 1010, 1996, 2069, 3185, 2029, 8539, 2216, 2093, 5889, 2362, 2003, 1996, 8053, 14485, 5869, 7136, 3203, 1006, 3339, 1007, 2021, 2054, 2008, 2028, 2038, 2000, 2079, 2007, 1996, 11351, 2697, 6071, 2003, 10334, 1005, 1055, 3984, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2130, 2061, 1010, 2144, 1045, 3825, 1002, 1015, 1012, 2753, 2005, 2049, 12635, 1998, 1045, 2001, 1999, 1037, 6071, 1011, 3185, 6888, 2151, 14406, 1010, 1045, 2700, 2000, 3422, 1996, 3185, 7539, 1998, 1010, 9805, 2361, 1010, 2009, 24646, 8950, 999, 4237, 2013, 1996, 2755, 2008, 2009, 2018, 1037, 2053, 1011, 2171, 3459, 1998, 2019, 10812, 3626, 1010, 2019, 4895, 23738, 11905, 6321, 5515, 4509, 2250, 2001, 5710, 2013, 2661, 2185, 1998, 1996, 2087, 1045, 2071, 2079, 2007, 2009, 2003, 4756, 2012, 1996, 16113, 1011, 2066, 3653, 29048, 2015, 1998, 1010, 15734, 2061, 1010, 2012, 1996, 9507, 7028, 27440, 1997, 2048, 22822, 12356, 3913, 7875, 12166, 1011, 13988, 1011, 4049, 5608, 2040, 4703, 5490, 6692, 11362, 2426, 3209, 2007, 1996, 2022, 7606, 2098, 2334, 6458, 2559, 2006, 1012, 1996, 6492, 2993, 1037, 20228, 2229, 10735, 21159, 1045, 1012, 1041, 1012, 2431, 1011, 15799, 1013, 2431, 1011, 3869, 2003, 29238, 2135, 3651, 1006, 8100, 1007, 2021, 1010, 2004, 2018, 2042, 1996, 2553, 2007, 1996, 5016, 15020, 1006, 3890, 1007, 2029, 1045, 1005, 2310, 2036, 2074, 2464, 1010, 2023, 2134, 1005, 1056, 4025, 2000, 8572, 1996, 2143, 1011, 11153, 3904, 2004, 2027, 13109, 4887, 3372, 2009, 2004, 2172, 2004, 2027, 2064, 1010, 2926, 2076, 1996, 3185, 1005, 1055, 2117, 2431, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(dataset[\"train\"][0][\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    temp = tokenizer(batch[\"review\"], padding=True, truncation=True, max_length=300)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 35000/35000 [00:07<00:00, 4380.13 examples/s]\n",
      "Map: 100%|██████████| 15000/15000 [00:03<00:00, 4340.34 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'sentiment', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 35000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review', 'sentiment', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.map(tokenize, batched=True, batch_size=256)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "id2label = {0: \"negative\", 1: \"positive\"}\n",
    "\n",
    "tr_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model,\n",
    "    num_labels=len(label2id),\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"train_dir\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    eval_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=tr_model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/dev/lib/python3.10/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/dev/lib/python3.10/site-packages/transformers/trainer.py:2284\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2285\u001b[0m ):\n\u001b[1;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text_classification.TextClassificationPipeline at 0x334198eb0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"tinybert-sentiment-analysis\", device=device)\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'positive', 'score': 0.8108767867088318},\n",
       " {'label': 'negative', 'score': 0.9376280307769775},\n",
       " {'label': 'negative', 'score': 0.9141314029693604},\n",
       " {'label': 'negative', 'score': 0.8275437951087952}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\"This movie is crazy...\", \"shouldn't have seen this one\", \"should have seen earlier\", \"should have brought my friends with me\"]\n",
    "classifier(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success to create bucket (jihwan-mlops)\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "bucket_name = \"jihwan-mlops\"\n",
    "\n",
    "def create_bucket(bucket_name):\n",
    "    try:\n",
    "        s3.create_bucket(\n",
    "            Bucket=bucket_name,\n",
    "            CreateBucketConfiguration={\"LocationConstraint\": \"ap-northeast-2\"}\n",
    "        )\n",
    "        print(f\"Success to create bucket ({bucket_name})\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "create_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '2Q0AJ4H5XSQ8DB24',\n",
       "  'HostId': 'NWyRlw4yYX7BNf5Leu0fmkBt2lBBGzP32tJVVztSq44Z5h/yiLZf7+KnhLgkKntujhLfDXn96DI=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'NWyRlw4yYX7BNf5Leu0fmkBt2lBBGzP32tJVVztSq44Z5h/yiLZf7+KnhLgkKntujhLfDXn96DI=',\n",
       "   'x-amz-request-id': '2Q0AJ4H5XSQ8DB24',\n",
       "   'date': 'Mon, 23 Sep 2024 07:20:43 GMT',\n",
       "   'content-type': 'application/xml',\n",
       "   'transfer-encoding': 'chunked',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'Buckets': [{'Name': 'jihwan-mlops',\n",
       "   'CreationDate': datetime.datetime(2024, 9, 23, 7, 20, 13, tzinfo=tzutc())}],\n",
       " 'Owner': {'ID': '130f68431ef941a9ebdb00664f815794a21d0b67da22a5b8a2af695f6cf4b191'}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3.list_buckets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tinybert-sentiment-analysis/model.safetensors\n",
      "tinybert-sentiment-analysis/tokenizer_config.json\n",
      "tinybert-sentiment-analysis/special_tokens_map.json\n",
      "tinybert-sentiment-analysis/config.json\n",
      "tinybert-sentiment-analysis/tokenizer.json\n",
      "tinybert-sentiment-analysis/training_args.bin\n",
      "tinybert-sentiment-analysis/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for root, dir, files in os.walk(\"tinybert-sentiment-analysis\"):\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(root, file_name)\n",
    "        print(file_path)\n",
    "        s3.upload_file(file_path, bucket_name, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
